{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training a Multi Layered Perceptron (MLP) using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workbook we will use the python library Tensorflow to implement an MLP. We will implement MLPs for classification as a way of dipping into Tensorflow. We will also cover considerations for training such as batch sizes and learning rates as well as ways to avoid overfitting. We will also looking at the training loss output as well as saving and loading models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a code cell, click on the cell the press \"Shift + Enter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "#Want to use version of Tensorflow > 2.0\n",
    "print('Using Tensorflow version %s' % tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data\n",
    "\n",
    "Let's generate a dataset, consisting of two data types which we call signal and background. Each data type is normally generated around a point in the x-y plane. Distinguishing signal from backgorund in this case is a very simple problem. You could use PDFs and likelihoods to identify signal and backgound. However for the purpose of this tutorial we will build a very simply MLP classifier to identify events as signal or background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets\n",
    "num_events = 10000\n",
    "\n",
    "#Signal x and y mean values\n",
    "signal_mean = [1.0, 1.0]\n",
    "#Signal x and y values are uncorrelated\n",
    "signal_cov = [[1.0, 0.0],\n",
    "              [0.0, 1.0]]\n",
    "\n",
    "#Generate a training and validation sample\n",
    "signal_train = np.random.multivariate_normal(\n",
    "        signal_mean, signal_cov, num_events)\n",
    "signal_val = np.random.multivariate_normal(\n",
    "        signal_mean, signal_cov, num_events)\n",
    "\n",
    "#Background x and y mean values\n",
    "background_mean = [-1.0, -1.0]\n",
    "#Background x and y values are uncorrelated\n",
    "background_cov = [[1.0, 0.0],\n",
    "                  [0.0, 1.0]]\n",
    "\n",
    "#Generate a training and validation sample\n",
    "background_train = np.random.multivariate_normal(\n",
    "        background_mean, background_cov, num_events)\n",
    "background_val = np.random.multivariate_normal(\n",
    "        background_mean, background_cov, num_events)\n",
    "\n",
    "#Add the signal and background samples\n",
    "data_train = np.vstack([signal_train, background_train])\n",
    "labels_train = np.vstack([np.ones((num_events, 1)), np.zeros((num_events, 1))])\n",
    "\n",
    "#Add the signal and background samples\n",
    "data_val = np.vstack([signal_val, background_val])\n",
    "labels_val = np.vstack([np.ones((num_events, 1)), np.zeros((num_events, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the datasets generated\n",
    "range_ = ((-3, 3), (-3, 3))\n",
    "plt.figure(0, figsize=(8,4))\n",
    "plt.subplot(1,2,1); plt.title(\"Signal\")\n",
    "plt.xlabel(\"x\"), plt.ylabel(\"y\")\n",
    "plt.hist2d(signal_train[:,0], signal_train[:,1],\n",
    "        range=range_, bins=20, cmap=cm.coolwarm)\n",
    "plt.subplot(1,2,2); plt.title(\"Background\")\n",
    "plt.hist2d(background_train[:,0], background_train[:,1],\n",
    "        range=range_, bins=20, cmap=cm.coolwarm)\n",
    "plt.xlabel(\"x\"), plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a simple problem we can build a simple MLP to identify signal and background events \n",
    "\n",
    "An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that can use a nonlinear activation function\n",
    "\n",
    "\n",
    "![title](images/mlp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Tensorflows sequential API. We pass the layers to the API sequentially\n",
    "#Construct Neural Net\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\", input_dim=2))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to look at the architecture of your neural net to check it makes sense. Do you have the total number of parameters you would expect to have. \n",
    "\n",
    "Expected number of parameters: each input is connected to a node in the first hidden layer by a weight. So that is 2 inputs connected to 100 nodes = 200 weights. Each node has a bias, so 100 biases. Therefore that is 200 weights + 100 biases = 300 parameters at the first layer\n",
    "\n",
    "The 100 nodes are connected to 1 ouput node, so that is 100 weights. The output node has a bias to the number of parameters here is 100. Therefore the total number of parameters required to construct this neural net is 401\n",
    "\n",
    "Use the model's method .summary() to see a breakdown of your neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 401\n",
      "Trainable params: 401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set loss function and optimiser. As this is a classification neural net we want to use binary cross entropy as the loss function. We will use Adam as the optimiser. The Adam optimiser is based on gradient decent but has a more sophisticated adaption of learning rates for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss function and optimiser\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on data. Define the dataset you want to train on and the validation set you want to validate the training against. Set the number of epochs (the number of iterations over the dataset). Set the batch size. This is a number/size of events smaller than your dataset that you run over and update the weights of your neural net. You run over multiples of batch sizes until you have run over all your events in your dataset. That is one epoch.  The batch size can actually be quite important. The main benefit of large batch sizes is that the training algorithm will see more instances per calculation. If you have the hardware large batch sizes are usually recommended. However large batch sizes can lead to instabilities. Particularly at the start of trainin.  A small batch size can be good if you are low on computer memory or want to avoid getting stuck in a local minima. Too small a batch size can make converging on optimal weighs slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.6399 - accuracy: 0.7311 - val_loss: 0.6290 - val_accuracy: 0.7882\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.6289 - accuracy: 0.7908 - val_loss: 0.6183 - val_accuracy: 0.8740\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.6182 - accuracy: 0.8762 - val_loss: 0.6077 - val_accuracy: 0.8782\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.6077 - accuracy: 0.8804 - val_loss: 0.5974 - val_accuracy: 0.8831\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.5974 - accuracy: 0.8838 - val_loss: 0.5872 - val_accuracy: 0.8857\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.5872 - accuracy: 0.8871 - val_loss: 0.5773 - val_accuracy: 0.8885\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.5773 - accuracy: 0.8894 - val_loss: 0.5675 - val_accuracy: 0.8916\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.5675 - accuracy: 0.8920 - val_loss: 0.5579 - val_accuracy: 0.8939\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.5580 - accuracy: 0.8942 - val_loss: 0.5486 - val_accuracy: 0.8965\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.5486 - accuracy: 0.8964 - val_loss: 0.5394 - val_accuracy: 0.8990\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.5395 - accuracy: 0.8978 - val_loss: 0.5304 - val_accuracy: 0.9004\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5305 - accuracy: 0.8994 - val_loss: 0.5217 - val_accuracy: 0.9017\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.5218 - accuracy: 0.9017 - val_loss: 0.5131 - val_accuracy: 0.9032\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.5132 - accuracy: 0.9029 - val_loss: 0.5047 - val_accuracy: 0.9038\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.5049 - accuracy: 0.9041 - val_loss: 0.4966 - val_accuracy: 0.9044\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.4967 - accuracy: 0.9051 - val_loss: 0.4886 - val_accuracy: 0.9050\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.4887 - accuracy: 0.9060 - val_loss: 0.4808 - val_accuracy: 0.9061\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4810 - accuracy: 0.9069 - val_loss: 0.4732 - val_accuracy: 0.9071\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.4734 - accuracy: 0.9082 - val_loss: 0.4658 - val_accuracy: 0.9077\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.4659 - accuracy: 0.9086 - val_loss: 0.4585 - val_accuracy: 0.9087\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.4587 - accuracy: 0.9095 - val_loss: 0.4515 - val_accuracy: 0.9093\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.4516 - accuracy: 0.9104 - val_loss: 0.4446 - val_accuracy: 0.9103\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.4447 - accuracy: 0.9111 - val_loss: 0.4378 - val_accuracy: 0.9107\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.4380 - accuracy: 0.9119 - val_loss: 0.4313 - val_accuracy: 0.9114\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4314 - accuracy: 0.9122 - val_loss: 0.4249 - val_accuracy: 0.9116\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4250 - accuracy: 0.9128 - val_loss: 0.4186 - val_accuracy: 0.9119\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.4188 - accuracy: 0.9135 - val_loss: 0.4125 - val_accuracy: 0.9122\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.4127 - accuracy: 0.9138 - val_loss: 0.4066 - val_accuracy: 0.9126\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.4068 - accuracy: 0.9138 - val_loss: 0.4008 - val_accuracy: 0.9132\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4010 - accuracy: 0.9143 - val_loss: 0.3952 - val_accuracy: 0.9136\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.3953 - accuracy: 0.9144 - val_loss: 0.3897 - val_accuracy: 0.9137\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.3899 - accuracy: 0.9145 - val_loss: 0.3844 - val_accuracy: 0.9141\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.3845 - accuracy: 0.9149 - val_loss: 0.3792 - val_accuracy: 0.9143\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.3793 - accuracy: 0.9155 - val_loss: 0.3741 - val_accuracy: 0.9144\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.3742 - accuracy: 0.9157 - val_loss: 0.3692 - val_accuracy: 0.9144\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.3693 - accuracy: 0.9162 - val_loss: 0.3644 - val_accuracy: 0.9144\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.3645 - accuracy: 0.9165 - val_loss: 0.3597 - val_accuracy: 0.9149\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3598 - accuracy: 0.9166 - val_loss: 0.3551 - val_accuracy: 0.9147\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.3552 - accuracy: 0.9168 - val_loss: 0.3507 - val_accuracy: 0.9148\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3508 - accuracy: 0.9173 - val_loss: 0.3464 - val_accuracy: 0.9150\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3465 - accuracy: 0.9172 - val_loss: 0.3422 - val_accuracy: 0.9153\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.3423 - accuracy: 0.9172 - val_loss: 0.3381 - val_accuracy: 0.9154\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.3382 - accuracy: 0.9174 - val_loss: 0.3342 - val_accuracy: 0.9155\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.3342 - accuracy: 0.9175 - val_loss: 0.3303 - val_accuracy: 0.9157\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.3303 - accuracy: 0.9178 - val_loss: 0.3265 - val_accuracy: 0.9158\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.3266 - accuracy: 0.9179 - val_loss: 0.3229 - val_accuracy: 0.9161\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.3229 - accuracy: 0.9180 - val_loss: 0.3193 - val_accuracy: 0.9159\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.3193 - accuracy: 0.9180 - val_loss: 0.3159 - val_accuracy: 0.9162\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.3159 - accuracy: 0.9182 - val_loss: 0.3125 - val_accuracy: 0.9164\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3125 - accuracy: 0.9184 - val_loss: 0.3092 - val_accuracy: 0.9165\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.3092 - accuracy: 0.9187 - val_loss: 0.3060 - val_accuracy: 0.9165\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.3060 - accuracy: 0.9188 - val_loss: 0.3029 - val_accuracy: 0.9164\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.3029 - accuracy: 0.9190 - val_loss: 0.2999 - val_accuracy: 0.9165\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2999 - accuracy: 0.9191 - val_loss: 0.2970 - val_accuracy: 0.9165\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.2970 - accuracy: 0.9194 - val_loss: 0.2942 - val_accuracy: 0.9165\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.2941 - accuracy: 0.9194 - val_loss: 0.2914 - val_accuracy: 0.9168\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2913 - accuracy: 0.9197 - val_loss: 0.2887 - val_accuracy: 0.9171\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2886 - accuracy: 0.9200 - val_loss: 0.2861 - val_accuracy: 0.9172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.2860 - accuracy: 0.9200 - val_loss: 0.2835 - val_accuracy: 0.9172\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.2834 - accuracy: 0.9202 - val_loss: 0.2810 - val_accuracy: 0.9174\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.2809 - accuracy: 0.9203 - val_loss: 0.2786 - val_accuracy: 0.9176\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2785 - accuracy: 0.9204 - val_loss: 0.2763 - val_accuracy: 0.9175\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2762 - accuracy: 0.9203 - val_loss: 0.2740 - val_accuracy: 0.9175\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2739 - accuracy: 0.9204 - val_loss: 0.2718 - val_accuracy: 0.9175\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.2716 - accuracy: 0.9207 - val_loss: 0.2696 - val_accuracy: 0.9174\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.2695 - accuracy: 0.9209 - val_loss: 0.2675 - val_accuracy: 0.9175\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.2674 - accuracy: 0.9209 - val_loss: 0.2655 - val_accuracy: 0.9174\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2653 - accuracy: 0.9208 - val_loss: 0.2635 - val_accuracy: 0.9175\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.2633 - accuracy: 0.9208 - val_loss: 0.2616 - val_accuracy: 0.9178\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.2614 - accuracy: 0.9208 - val_loss: 0.2597 - val_accuracy: 0.9180\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.2595 - accuracy: 0.9208 - val_loss: 0.2579 - val_accuracy: 0.9180\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.2577 - accuracy: 0.9209 - val_loss: 0.2561 - val_accuracy: 0.9180\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2559 - accuracy: 0.9210 - val_loss: 0.2544 - val_accuracy: 0.9182\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2542 - accuracy: 0.9211 - val_loss: 0.2527 - val_accuracy: 0.9182\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.2525 - accuracy: 0.9212 - val_loss: 0.2511 - val_accuracy: 0.9181\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.2508 - accuracy: 0.9213 - val_loss: 0.2495 - val_accuracy: 0.9182\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.2492 - accuracy: 0.9213 - val_loss: 0.2479 - val_accuracy: 0.9182\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.2477 - accuracy: 0.9214 - val_loss: 0.2464 - val_accuracy: 0.9183\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2462 - accuracy: 0.9215 - val_loss: 0.2450 - val_accuracy: 0.9183\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2447 - accuracy: 0.9218 - val_loss: 0.2436 - val_accuracy: 0.9183\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2433 - accuracy: 0.9218 - val_loss: 0.2422 - val_accuracy: 0.9184\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2419 - accuracy: 0.9217 - val_loss: 0.2408 - val_accuracy: 0.9186\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.2405 - accuracy: 0.9218 - val_loss: 0.2395 - val_accuracy: 0.9187\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.2392 - accuracy: 0.9215 - val_loss: 0.2383 - val_accuracy: 0.9187\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.2379 - accuracy: 0.9217 - val_loss: 0.2370 - val_accuracy: 0.9188\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.2367 - accuracy: 0.9219 - val_loss: 0.2358 - val_accuracy: 0.9187\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.2355 - accuracy: 0.9219 - val_loss: 0.2347 - val_accuracy: 0.9190\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.2343 - accuracy: 0.9220 - val_loss: 0.2335 - val_accuracy: 0.9190\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.2332 - accuracy: 0.9220 - val_loss: 0.2324 - val_accuracy: 0.9190\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.2320 - accuracy: 0.9219 - val_loss: 0.2313 - val_accuracy: 0.9192\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.2310 - accuracy: 0.9220 - val_loss: 0.2303 - val_accuracy: 0.9193\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2299 - accuracy: 0.9220 - val_loss: 0.2293 - val_accuracy: 0.9194\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2289 - accuracy: 0.9220 - val_loss: 0.2283 - val_accuracy: 0.9196\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2279 - accuracy: 0.9221 - val_loss: 0.2273 - val_accuracy: 0.9194\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.2269 - accuracy: 0.9221 - val_loss: 0.2264 - val_accuracy: 0.9196\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2260 - accuracy: 0.9221 - val_loss: 0.2255 - val_accuracy: 0.9196\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.2251 - accuracy: 0.9221 - val_loss: 0.2246 - val_accuracy: 0.9196\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2242 - accuracy: 0.9222 - val_loss: 0.2237 - val_accuracy: 0.9194\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.2233 - accuracy: 0.9222 - val_loss: 0.2229 - val_accuracy: 0.9194\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.2225 - accuracy: 0.9222 - val_loss: 0.2221 - val_accuracy: 0.9194\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.2216 - accuracy: 0.9222 - val_loss: 0.2213 - val_accuracy: 0.9194\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.2208 - accuracy: 0.9223 - val_loss: 0.2205 - val_accuracy: 0.9195\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2201 - accuracy: 0.9222 - val_loss: 0.2198 - val_accuracy: 0.9196\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.2193 - accuracy: 0.9221 - val_loss: 0.2191 - val_accuracy: 0.9197\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.2186 - accuracy: 0.9221 - val_loss: 0.2183 - val_accuracy: 0.9197\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.2179 - accuracy: 0.9220 - val_loss: 0.2177 - val_accuracy: 0.9197\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.2172 - accuracy: 0.9220 - val_loss: 0.2170 - val_accuracy: 0.9197\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.2165 - accuracy: 0.9221 - val_loss: 0.2163 - val_accuracy: 0.9200\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.2158 - accuracy: 0.9220 - val_loss: 0.2157 - val_accuracy: 0.9201\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.2152 - accuracy: 0.9219 - val_loss: 0.2151 - val_accuracy: 0.9201\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2146 - accuracy: 0.9219 - val_loss: 0.2145 - val_accuracy: 0.9201\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2140 - accuracy: 0.9219 - val_loss: 0.2139 - val_accuracy: 0.9201\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2134 - accuracy: 0.9219 - val_loss: 0.2133 - val_accuracy: 0.9203\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2128 - accuracy: 0.9219 - val_loss: 0.2128 - val_accuracy: 0.9203\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2123 - accuracy: 0.9219 - val_loss: 0.2123 - val_accuracy: 0.9202\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2117 - accuracy: 0.9219 - val_loss: 0.2117 - val_accuracy: 0.9201\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.2112 - accuracy: 0.9220 - val_loss: 0.2112 - val_accuracy: 0.9201\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.2107 - accuracy: 0.9222 - val_loss: 0.2107 - val_accuracy: 0.9201\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.2102 - accuracy: 0.9222 - val_loss: 0.2103 - val_accuracy: 0.9201\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2097 - accuracy: 0.9222 - val_loss: 0.2098 - val_accuracy: 0.9203\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2092 - accuracy: 0.9223 - val_loss: 0.2093 - val_accuracy: 0.9201\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2088 - accuracy: 0.9223 - val_loss: 0.2089 - val_accuracy: 0.9202\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2083 - accuracy: 0.9223 - val_loss: 0.2085 - val_accuracy: 0.9201\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.2079 - accuracy: 0.9224 - val_loss: 0.2080 - val_accuracy: 0.9203\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2075 - accuracy: 0.9225 - val_loss: 0.2076 - val_accuracy: 0.9204\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2070 - accuracy: 0.9226 - val_loss: 0.2072 - val_accuracy: 0.9204\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.2066 - accuracy: 0.9226 - val_loss: 0.2069 - val_accuracy: 0.9204\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.2063 - accuracy: 0.9226 - val_loss: 0.2065 - val_accuracy: 0.9204\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.2059 - accuracy: 0.9226 - val_loss: 0.2061 - val_accuracy: 0.9204\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.2055 - accuracy: 0.9226 - val_loss: 0.2058 - val_accuracy: 0.9204\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.2052 - accuracy: 0.9225 - val_loss: 0.2054 - val_accuracy: 0.9205\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.2048 - accuracy: 0.9226 - val_loss: 0.2051 - val_accuracy: 0.9205\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2045 - accuracy: 0.9226 - val_loss: 0.2048 - val_accuracy: 0.9206\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.2041 - accuracy: 0.9226 - val_loss: 0.2044 - val_accuracy: 0.9207\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.2038 - accuracy: 0.9225 - val_loss: 0.2041 - val_accuracy: 0.9207\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.2035 - accuracy: 0.9224 - val_loss: 0.2038 - val_accuracy: 0.9207\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.2032 - accuracy: 0.9225 - val_loss: 0.2035 - val_accuracy: 0.9208\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.2029 - accuracy: 0.9226 - val_loss: 0.2033 - val_accuracy: 0.9208\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2026 - accuracy: 0.9226 - val_loss: 0.2030 - val_accuracy: 0.9208\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.2023 - accuracy: 0.9226 - val_loss: 0.2027 - val_accuracy: 0.9209\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.2021 - accuracy: 0.9226 - val_loss: 0.2025 - val_accuracy: 0.9209\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2018 - accuracy: 0.9226 - val_loss: 0.2022 - val_accuracy: 0.9211\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2015 - accuracy: 0.9227 - val_loss: 0.2020 - val_accuracy: 0.9211\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.2013 - accuracy: 0.9226 - val_loss: 0.2017 - val_accuracy: 0.9211\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2010 - accuracy: 0.9226 - val_loss: 0.2015 - val_accuracy: 0.9212\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.2008 - accuracy: 0.9226 - val_loss: 0.2013 - val_accuracy: 0.9212\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.2006 - accuracy: 0.9226 - val_loss: 0.2010 - val_accuracy: 0.9212\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.2004 - accuracy: 0.9227 - val_loss: 0.2008 - val_accuracy: 0.9212\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.2001 - accuracy: 0.9226 - val_loss: 0.2006 - val_accuracy: 0.9212\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1999 - accuracy: 0.9226 - val_loss: 0.2004 - val_accuracy: 0.9212\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.1997 - accuracy: 0.9226 - val_loss: 0.2002 - val_accuracy: 0.9212\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1995 - accuracy: 0.9225 - val_loss: 0.2000 - val_accuracy: 0.9212\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1993 - accuracy: 0.9225 - val_loss: 0.1998 - val_accuracy: 0.9212\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1991 - accuracy: 0.9226 - val_loss: 0.1997 - val_accuracy: 0.9212\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1989 - accuracy: 0.9226 - val_loss: 0.1995 - val_accuracy: 0.9211\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.1988 - accuracy: 0.9226 - val_loss: 0.1993 - val_accuracy: 0.9212\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1986 - accuracy: 0.9226 - val_loss: 0.1991 - val_accuracy: 0.9211\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1984 - accuracy: 0.9226 - val_loss: 0.1990 - val_accuracy: 0.9212\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1983 - accuracy: 0.9226 - val_loss: 0.1988 - val_accuracy: 0.9212\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.1981 - accuracy: 0.9226 - val_loss: 0.1987 - val_accuracy: 0.9212\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1979 - accuracy: 0.9226 - val_loss: 0.1985 - val_accuracy: 0.9212\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1978 - accuracy: 0.9226 - val_loss: 0.1984 - val_accuracy: 0.9212\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1976 - accuracy: 0.9226 - val_loss: 0.1982 - val_accuracy: 0.9212\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.1975 - accuracy: 0.9225 - val_loss: 0.1981 - val_accuracy: 0.9212\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1974 - accuracy: 0.9225 - val_loss: 0.1980 - val_accuracy: 0.9212\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.1972 - accuracy: 0.9225 - val_loss: 0.1978 - val_accuracy: 0.9212\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1971 - accuracy: 0.9224 - val_loss: 0.1977 - val_accuracy: 0.9212\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.1970 - accuracy: 0.9224 - val_loss: 0.1976 - val_accuracy: 0.9212\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.1968 - accuracy: 0.9224 - val_loss: 0.1975 - val_accuracy: 0.9212\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1967 - accuracy: 0.9223 - val_loss: 0.1974 - val_accuracy: 0.9212\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.1966 - accuracy: 0.9223 - val_loss: 0.1972 - val_accuracy: 0.9212\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.1965 - accuracy: 0.9223 - val_loss: 0.1971 - val_accuracy: 0.9212\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 156ms/step - loss: 0.1964 - accuracy: 0.9222 - val_loss: 0.1970 - val_accuracy: 0.9213\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.1963 - accuracy: 0.9222 - val_loss: 0.1969 - val_accuracy: 0.9213\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.1961 - accuracy: 0.9222 - val_loss: 0.1968 - val_accuracy: 0.9212\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.1960 - accuracy: 0.9222 - val_loss: 0.1967 - val_accuracy: 0.9212\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.1959 - accuracy: 0.9222 - val_loss: 0.1966 - val_accuracy: 0.9212\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1958 - accuracy: 0.9223 - val_loss: 0.1965 - val_accuracy: 0.9212\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1958 - accuracy: 0.9224 - val_loss: 0.1964 - val_accuracy: 0.9212\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.1957 - accuracy: 0.9224 - val_loss: 0.1964 - val_accuracy: 0.9212\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1956 - accuracy: 0.9223 - val_loss: 0.1963 - val_accuracy: 0.9212\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1955 - accuracy: 0.9223 - val_loss: 0.1962 - val_accuracy: 0.9212\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.1954 - accuracy: 0.9223 - val_loss: 0.1961 - val_accuracy: 0.9211\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1953 - accuracy: 0.9223 - val_loss: 0.1960 - val_accuracy: 0.9211\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1952 - accuracy: 0.9223 - val_loss: 0.1960 - val_accuracy: 0.9212\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.1952 - accuracy: 0.9223 - val_loss: 0.1959 - val_accuracy: 0.9212\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.1951 - accuracy: 0.9223 - val_loss: 0.1958 - val_accuracy: 0.9211\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1950 - accuracy: 0.9223 - val_loss: 0.1957 - val_accuracy: 0.9211\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1949 - accuracy: 0.9224 - val_loss: 0.1957 - val_accuracy: 0.9211\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1949 - accuracy: 0.9223 - val_loss: 0.1956 - val_accuracy: 0.9211\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.1948 - accuracy: 0.9224 - val_loss: 0.1956 - val_accuracy: 0.9210\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.1947 - accuracy: 0.9224 - val_loss: 0.1955 - val_accuracy: 0.9210\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1947 - accuracy: 0.9224 - val_loss: 0.1954 - val_accuracy: 0.9210\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1946 - accuracy: 0.9224 - val_loss: 0.1954 - val_accuracy: 0.9211\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.1946 - accuracy: 0.9223 - val_loss: 0.1953 - val_accuracy: 0.9211\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.1945 - accuracy: 0.9223 - val_loss: 0.1953 - val_accuracy: 0.9211\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1944 - accuracy: 0.9223 - val_loss: 0.1952 - val_accuracy: 0.9211\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.1944 - accuracy: 0.9223 - val_loss: 0.1952 - val_accuracy: 0.9211\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1943 - accuracy: 0.9223 - val_loss: 0.1951 - val_accuracy: 0.9211\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1943 - accuracy: 0.9223 - val_loss: 0.1951 - val_accuracy: 0.9212\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "#Not worried about memory or local minima\n",
    "batchSize = len(data_train)\n",
    "\n",
    "#Train on data\n",
    "history = model.fit(data_train, labels_train,\n",
    "          validation_data=(data_val, labels_val),\n",
    "          batch_size=batchSize,\n",
    "          epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% accurracy. Not bad! Be careful with accuracy measurments. It is easy to measure high accuracy if your validation or test sample mainly has one category of data. Not the case here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss of the neural net as a function of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fab6c720c10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', color='red', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about saving the model. Also at what point do save the model. This is an ideal example where the loss is smooth and decreases monotonically. You'll find this is rarely the case. What I tend to do is save the model each time the validation loss reaches a minimum. You overtrain your neural net if you use the training loss as a metric for saving your model. Let's use a callback to save the model. A callback kets you specify a list of objects that will be called during training. We can use a callback to save the model every time the validation loss is at a minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9224\n",
      "Epoch 00001: val_loss improved from inf to 0.19501, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.1942 - accuracy: 0.9224 - val_loss: 0.1950 - val_accuracy: 0.9212\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9224\n",
      "Epoch 00002: val_loss improved from 0.19501 to 0.19496, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1942 - accuracy: 0.9224 - val_loss: 0.1950 - val_accuracy: 0.9212\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9225\n",
      "Epoch 00003: val_loss improved from 0.19496 to 0.19492, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.1941 - accuracy: 0.9225 - val_loss: 0.1949 - val_accuracy: 0.9211\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9225\n",
      "Epoch 00004: val_loss improved from 0.19492 to 0.19488, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.1941 - accuracy: 0.9225 - val_loss: 0.1949 - val_accuracy: 0.9211\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9225\n",
      "Epoch 00005: val_loss improved from 0.19488 to 0.19483, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.1940 - accuracy: 0.9225 - val_loss: 0.1948 - val_accuracy: 0.9211\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9225\n",
      "Epoch 00006: val_loss improved from 0.19483 to 0.19479, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.1940 - accuracy: 0.9225 - val_loss: 0.1948 - val_accuracy: 0.9211\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9225\n",
      "Epoch 00007: val_loss improved from 0.19479 to 0.19475, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.1939 - accuracy: 0.9225 - val_loss: 0.1948 - val_accuracy: 0.9212\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9225\n",
      "Epoch 00008: val_loss improved from 0.19475 to 0.19471, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1939 - accuracy: 0.9225 - val_loss: 0.1947 - val_accuracy: 0.9211\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9225\n",
      "Epoch 00009: val_loss improved from 0.19471 to 0.19468, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.1939 - accuracy: 0.9225 - val_loss: 0.1947 - val_accuracy: 0.9211\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9225\n",
      "Epoch 00010: val_loss improved from 0.19468 to 0.19464, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.1938 - accuracy: 0.9225 - val_loss: 0.1946 - val_accuracy: 0.9211\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9225\n",
      "Epoch 00011: val_loss improved from 0.19464 to 0.19460, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1938 - accuracy: 0.9225 - val_loss: 0.1946 - val_accuracy: 0.9211\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9225\n",
      "Epoch 00012: val_loss improved from 0.19460 to 0.19457, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1937 - accuracy: 0.9225 - val_loss: 0.1946 - val_accuracy: 0.9211\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9224\n",
      "Epoch 00013: val_loss improved from 0.19457 to 0.19454, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.1937 - accuracy: 0.9224 - val_loss: 0.1945 - val_accuracy: 0.9211\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9224\n",
      "Epoch 00014: val_loss improved from 0.19454 to 0.19450, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.1937 - accuracy: 0.9224 - val_loss: 0.1945 - val_accuracy: 0.9211\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9224\n",
      "Epoch 00015: val_loss improved from 0.19450 to 0.19447, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.1936 - accuracy: 0.9224 - val_loss: 0.1945 - val_accuracy: 0.9211\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9224\n",
      "Epoch 00016: val_loss improved from 0.19447 to 0.19444, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1936 - accuracy: 0.9224 - val_loss: 0.1944 - val_accuracy: 0.9213\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9225\n",
      "Epoch 00017: val_loss improved from 0.19444 to 0.19441, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1936 - accuracy: 0.9225 - val_loss: 0.1944 - val_accuracy: 0.9213\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9225\n",
      "Epoch 00018: val_loss improved from 0.19441 to 0.19438, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.1935 - accuracy: 0.9225 - val_loss: 0.1944 - val_accuracy: 0.9213\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9225\n",
      "Epoch 00019: val_loss improved from 0.19438 to 0.19436, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1935 - accuracy: 0.9225 - val_loss: 0.1944 - val_accuracy: 0.9212\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9225\n",
      "Epoch 00020: val_loss improved from 0.19436 to 0.19433, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.1935 - accuracy: 0.9225 - val_loss: 0.1943 - val_accuracy: 0.9212\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9225\n",
      "Epoch 00021: val_loss improved from 0.19433 to 0.19430, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1935 - accuracy: 0.9225 - val_loss: 0.1943 - val_accuracy: 0.9212\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9225\n",
      "Epoch 00022: val_loss improved from 0.19430 to 0.19428, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1934 - accuracy: 0.9225 - val_loss: 0.1943 - val_accuracy: 0.9212\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9225\n",
      "Epoch 00023: val_loss improved from 0.19428 to 0.19425, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.1934 - accuracy: 0.9225 - val_loss: 0.1943 - val_accuracy: 0.9212\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9225\n",
      "Epoch 00024: val_loss improved from 0.19425 to 0.19423, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1934 - accuracy: 0.9225 - val_loss: 0.1942 - val_accuracy: 0.9212\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9225\n",
      "Epoch 00025: val_loss improved from 0.19423 to 0.19421, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1934 - accuracy: 0.9225 - val_loss: 0.1942 - val_accuracy: 0.9212\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9225\n",
      "Epoch 00026: val_loss improved from 0.19421 to 0.19418, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.1933 - accuracy: 0.9225 - val_loss: 0.1942 - val_accuracy: 0.9212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9225\n",
      "Epoch 00027: val_loss improved from 0.19418 to 0.19416, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1933 - accuracy: 0.9225 - val_loss: 0.1942 - val_accuracy: 0.9212\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9225\n",
      "Epoch 00028: val_loss improved from 0.19416 to 0.19414, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1933 - accuracy: 0.9225 - val_loss: 0.1941 - val_accuracy: 0.9211\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9225\n",
      "Epoch 00029: val_loss improved from 0.19414 to 0.19412, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1933 - accuracy: 0.9225 - val_loss: 0.1941 - val_accuracy: 0.9211\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9225\n",
      "Epoch 00030: val_loss improved from 0.19412 to 0.19410, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.1932 - accuracy: 0.9225 - val_loss: 0.1941 - val_accuracy: 0.9211\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9226\n",
      "Epoch 00031: val_loss improved from 0.19410 to 0.19408, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.1932 - accuracy: 0.9226 - val_loss: 0.1941 - val_accuracy: 0.9211\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9225\n",
      "Epoch 00032: val_loss improved from 0.19408 to 0.19406, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1932 - accuracy: 0.9225 - val_loss: 0.1941 - val_accuracy: 0.9211\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9225\n",
      "Epoch 00033: val_loss improved from 0.19406 to 0.19404, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.1932 - accuracy: 0.9225 - val_loss: 0.1940 - val_accuracy: 0.9212\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9225\n",
      "Epoch 00034: val_loss improved from 0.19404 to 0.19402, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.1931 - accuracy: 0.9225 - val_loss: 0.1940 - val_accuracy: 0.9212\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9226\n",
      "Epoch 00035: val_loss improved from 0.19402 to 0.19401, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1931 - accuracy: 0.9226 - val_loss: 0.1940 - val_accuracy: 0.9212\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9226\n",
      "Epoch 00036: val_loss improved from 0.19401 to 0.19399, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.1931 - accuracy: 0.9226 - val_loss: 0.1940 - val_accuracy: 0.9212\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9226\n",
      "Epoch 00037: val_loss improved from 0.19399 to 0.19397, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1931 - accuracy: 0.9226 - val_loss: 0.1940 - val_accuracy: 0.9212\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9226\n",
      "Epoch 00038: val_loss improved from 0.19397 to 0.19396, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.1931 - accuracy: 0.9226 - val_loss: 0.1940 - val_accuracy: 0.9212\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9226\n",
      "Epoch 00039: val_loss improved from 0.19396 to 0.19394, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.1931 - accuracy: 0.9226 - val_loss: 0.1939 - val_accuracy: 0.9212\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9226\n",
      "Epoch 00040: val_loss improved from 0.19394 to 0.19393, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.1930 - accuracy: 0.9226 - val_loss: 0.1939 - val_accuracy: 0.9212\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9225\n",
      "Epoch 00041: val_loss improved from 0.19393 to 0.19391, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1930 - accuracy: 0.9225 - val_loss: 0.1939 - val_accuracy: 0.9212\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9225\n",
      "Epoch 00042: val_loss improved from 0.19391 to 0.19390, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.1930 - accuracy: 0.9225 - val_loss: 0.1939 - val_accuracy: 0.9211\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9225\n",
      "Epoch 00043: val_loss improved from 0.19390 to 0.19388, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1930 - accuracy: 0.9225 - val_loss: 0.1939 - val_accuracy: 0.9210\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9225\n",
      "Epoch 00044: val_loss improved from 0.19388 to 0.19387, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.1930 - accuracy: 0.9225 - val_loss: 0.1939 - val_accuracy: 0.9210\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9224\n",
      "Epoch 00045: val_loss improved from 0.19387 to 0.19386, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1930 - accuracy: 0.9224 - val_loss: 0.1939 - val_accuracy: 0.9210\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9224\n",
      "Epoch 00046: val_loss improved from 0.19386 to 0.19384, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1929 - accuracy: 0.9224 - val_loss: 0.1938 - val_accuracy: 0.9210\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9225\n",
      "Epoch 00047: val_loss improved from 0.19384 to 0.19383, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.1929 - accuracy: 0.9225 - val_loss: 0.1938 - val_accuracy: 0.9210\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9225\n",
      "Epoch 00048: val_loss improved from 0.19383 to 0.19382, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1929 - accuracy: 0.9225 - val_loss: 0.1938 - val_accuracy: 0.9209\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9225\n",
      "Epoch 00049: val_loss improved from 0.19382 to 0.19381, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.1929 - accuracy: 0.9225 - val_loss: 0.1938 - val_accuracy: 0.9208\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9225\n",
      "Epoch 00050: val_loss improved from 0.19381 to 0.19380, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.1929 - accuracy: 0.9225 - val_loss: 0.1938 - val_accuracy: 0.9208\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9226\n",
      "Epoch 00051: val_loss improved from 0.19380 to 0.19379, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1929 - accuracy: 0.9226 - val_loss: 0.1938 - val_accuracy: 0.9209\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9226\n",
      "Epoch 00052: val_loss improved from 0.19379 to 0.19378, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1929 - accuracy: 0.9226 - val_loss: 0.1938 - val_accuracy: 0.9209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9226\n",
      "Epoch 00053: val_loss improved from 0.19378 to 0.19377, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1929 - accuracy: 0.9226 - val_loss: 0.1938 - val_accuracy: 0.9208\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00054: val_loss improved from 0.19377 to 0.19376, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1938 - val_accuracy: 0.9208\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00055: val_loss improved from 0.19376 to 0.19375, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00056: val_loss improved from 0.19375 to 0.19374, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00057: val_loss improved from 0.19374 to 0.19373, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00058: val_loss improved from 0.19373 to 0.19372, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00059: val_loss improved from 0.19372 to 0.19371, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00060: val_loss improved from 0.19371 to 0.19370, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9227\n",
      "Epoch 00061: val_loss improved from 0.19370 to 0.19369, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1928 - accuracy: 0.9227 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00062: val_loss improved from 0.19369 to 0.19369, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9226\n",
      "Epoch 00063: val_loss improved from 0.19369 to 0.19368, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.1928 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9208\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00064: val_loss improved from 0.19368 to 0.19367, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9209\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00065: val_loss improved from 0.19367 to 0.19366, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9209\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00066: val_loss improved from 0.19366 to 0.19366, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1937 - val_accuracy: 0.9209\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00067: val_loss improved from 0.19366 to 0.19365, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9208\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00068: val_loss improved from 0.19365 to 0.19364, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9208\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00069: val_loss improved from 0.19364 to 0.19364, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9208\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00070: val_loss improved from 0.19364 to 0.19363, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9208\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00071: val_loss improved from 0.19363 to 0.19362, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00072: val_loss improved from 0.19362 to 0.19362, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00073: val_loss improved from 0.19362 to 0.19361, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00074: val_loss improved from 0.19361 to 0.19361, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00075: val_loss improved from 0.19361 to 0.19360, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00076: val_loss improved from 0.19360 to 0.19360, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 00077: val_loss improved from 0.19360 to 0.19359, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00078: val_loss improved from 0.19359 to 0.19359, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00079: val_loss improved from 0.19359 to 0.19358, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9210\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00080: val_loss improved from 0.19358 to 0.19358, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9210\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00081: val_loss improved from 0.19358 to 0.19357, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9210\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00082: val_loss improved from 0.19357 to 0.19357, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00083: val_loss improved from 0.19357 to 0.19356, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00084: val_loss improved from 0.19356 to 0.19356, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00085: val_loss improved from 0.19356 to 0.19356, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00086: val_loss improved from 0.19356 to 0.19355, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1936 - val_accuracy: 0.9209\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00087: val_loss improved from 0.19355 to 0.19355, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00088: val_loss improved from 0.19355 to 0.19354, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00089: val_loss improved from 0.19354 to 0.19354, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00090: val_loss improved from 0.19354 to 0.19354, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00091: val_loss improved from 0.19354 to 0.19353, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00092: val_loss improved from 0.19353 to 0.19353, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00093: val_loss improved from 0.19353 to 0.19353, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00094: val_loss improved from 0.19353 to 0.19352, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00095: val_loss improved from 0.19352 to 0.19352, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9209\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00096: val_loss improved from 0.19352 to 0.19352, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9209\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9226\n",
      "Epoch 00097: val_loss improved from 0.19352 to 0.19351, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1926 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9209\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00098: val_loss improved from 0.19351 to 0.19351, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9209\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00099: val_loss improved from 0.19351 to 0.19351, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9209\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00100: val_loss improved from 0.19351 to 0.19350, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9209\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00101: val_loss improved from 0.19350 to 0.19350, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00102: val_loss improved from 0.19350 to 0.19350, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00103: val_loss improved from 0.19350 to 0.19350, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00104: val_loss improved from 0.19350 to 0.19349, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00105: val_loss improved from 0.19349 to 0.19349, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00106: val_loss improved from 0.19349 to 0.19349, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00107: val_loss improved from 0.19349 to 0.19349, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00108: val_loss improved from 0.19349 to 0.19348, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00109: val_loss improved from 0.19348 to 0.19348, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9208\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00110: val_loss improved from 0.19348 to 0.19348, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9207\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00111: val_loss improved from 0.19348 to 0.19348, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9207\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00112: val_loss improved from 0.19348 to 0.19347, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9207\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00113: val_loss improved from 0.19347 to 0.19347, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9206\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00114: val_loss improved from 0.19347 to 0.19347, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9206\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00115: val_loss improved from 0.19347 to 0.19347, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9206\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00116: val_loss improved from 0.19347 to 0.19347, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9206\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00117: val_loss improved from 0.19347 to 0.19346, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9206\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00118: val_loss improved from 0.19346 to 0.19346, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00119: val_loss improved from 0.19346 to 0.19346, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00120: val_loss improved from 0.19346 to 0.19346, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00121: val_loss improved from 0.19346 to 0.19346, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00122: val_loss improved from 0.19346 to 0.19345, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00123: val_loss improved from 0.19345 to 0.19345, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00124: val_loss improved from 0.19345 to 0.19345, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1935 - val_accuracy: 0.9205\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00125: val_loss improved from 0.19345 to 0.19345, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00126: val_loss improved from 0.19345 to 0.19345, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00127: val_loss improved from 0.19345 to 0.19345, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00128: val_loss improved from 0.19345 to 0.19344, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00129: val_loss improved from 0.19344 to 0.19344, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00130: val_loss improved from 0.19344 to 0.19344, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 00131: val_loss improved from 0.19344 to 0.19344, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.1925 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9226\n",
      "Epoch 00132: val_loss improved from 0.19344 to 0.19344, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.1924 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9226\n",
      "Epoch 00133: val_loss improved from 0.19344 to 0.19344, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1924 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9226\n",
      "Epoch 00134: val_loss improved from 0.19344 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.1924 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9226\n",
      "Epoch 00135: val_loss improved from 0.19343 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.1924 - accuracy: 0.9226 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00136: val_loss improved from 0.19343 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00137: val_loss improved from 0.19343 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00138: val_loss improved from 0.19343 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00139: val_loss improved from 0.19343 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00140: val_loss improved from 0.19343 to 0.19343, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00141: val_loss improved from 0.19343 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00142: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9225\n",
      "Epoch 00143: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1924 - accuracy: 0.9225 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00144: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00145: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00146: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00147: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00148: val_loss improved from 0.19342 to 0.19342, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00149: val_loss improved from 0.19342 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00150: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00151: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00152: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00153: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00154: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00155: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00156: val_loss improved from 0.19341 to 0.19341, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00157: val_loss improved from 0.19341 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00158: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00159: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00160: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00161: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00162: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00163: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00164: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9205\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00165: val_loss improved from 0.19340 to 0.19340, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00166: val_loss improved from 0.19340 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9224\n",
      "Epoch 00167: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.1924 - accuracy: 0.9224 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00168: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00169: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00170: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00171: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00172: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00173: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9223\n",
      "Epoch 00174: val_loss improved from 0.19339 to 0.19339, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1924 - accuracy: 0.9223 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00175: val_loss improved from 0.19339 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00176: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00177: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00178: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00179: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00180: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00181: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00182: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00183: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00184: val_loss improved from 0.19338 to 0.19338, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00185: val_loss improved from 0.19338 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9222\n",
      "Epoch 00186: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.1924 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00187: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00188: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00189: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00190: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00191: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9207\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00192: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00193: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00194: val_loss improved from 0.19337 to 0.19337, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00195: val_loss improved from 0.19337 to 0.19336, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9222\n",
      "Epoch 00196: val_loss improved from 0.19336 to 0.19336, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1923 - accuracy: 0.9222 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9221\n",
      "Epoch 00197: val_loss improved from 0.19336 to 0.19336, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.1923 - accuracy: 0.9221 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9221\n",
      "Epoch 00198: val_loss improved from 0.19336 to 0.19336, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1923 - accuracy: 0.9221 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9221\n",
      "Epoch 00199: val_loss improved from 0.19336 to 0.19336, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1923 - accuracy: 0.9221 - val_loss: 0.1934 - val_accuracy: 0.9208\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9221\n",
      "Epoch 00200: val_loss improved from 0.19336 to 0.19336, saving model to models/simple_mlp.h5\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.1923 - accuracy: 0.9221 - val_loss: 0.1934 - val_accuracy: 0.9208\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Specify a directory to save model\n",
    "\n",
    "model_filename = 'simple_mlp.h5'\n",
    "saved_model_directory = 'models'\n",
    "\n",
    "checkpoint_filename = os.path.join(saved_model_directory, model_filename)\n",
    "\n",
    "CHECK_FOLDER = os.path.isdir(saved_model_directory)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(saved_model_directory)\n",
    "    print(\"created folder : \", saved_model_directory)\n",
    "                \n",
    "#Create checkpoint\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filename,\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose = 1)\n",
    "\n",
    "        \n",
    "history = model.fit(data_train, labels_train,\n",
    "        validation_data=(data_val, labels_val),\n",
    "        batch_size=len(data_train),\n",
    "        epochs=num_epochs,\n",
    "        callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice your model continues learning from its previous parameters! Makes sense, you haven't rest the model parameters from the last training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a saved model\n",
    "\n",
    "Now we have saved the parameters we can load them at any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "model.load_weights(checkpoint_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification with a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the MNIST dataset, famous in machine learning examples. Specifically we can look at images of handwritten numbers 0-9 and use a MLP to identify them. This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the <a id='http://yann.lecun.com/exdb/mnist'>MNIST homepage</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label is: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "# Randomly look at one of the images\n",
    "random_ints =np.random.randint(len(x_test), size=1)\n",
    "image_index = random_ints[0]\n",
    "\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "\n",
    "print('Image label is: %i' % y_test[image_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an mlp to identify the images. The intenstiy of the pixels is represented as a value from 0 to 255. We should normalise the intensity. Also, the image is a 2D array so we should flatten it to a 1D array so the values can be input into a MLP. Lets build a simple MLP with a hidden layer containing 128 nodes and an output layer of 10 nodes. Again we can work out how many parameters it would take to build this mlp. \n",
    "\n",
    "The total number of inputs required for the mlp is (an input for each pixel) 28 x 28  = 784\n",
    "Total number of weights to connect each input to each node in the first layer is 784 x 128 = 100352\n",
    "Total number of biases in the first layer is 128.\n",
    "\n",
    "Total number of parameters for the first layer is = 100352 + 128 = 100480\n",
    "\n",
    "Total number of parameters for the outut layer is = number of weights + biases = (128 * 10) + 10 = 1290\n",
    "\n",
    "Total number of parametes =  101770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Intensity of pixels ranges from 0-255. We need to normalise them\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "#Rather than instantiate the node and use the .add method to keep adding input,layers, outputs, etc we can just state\n",
    "#components separated by a comma during instantiation\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would expect some kind of sigmoid activation function to be applied to the output. However we let the loss function SparseCategoricalCrossentropy take the output and use it to calculate the loss to best attribute probabilities to each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity of pixels ranges from 0-255. We need to normalise them\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#Split into training and validation samples\n",
    "x_train, x_val = x_train[0:55000], x_train[55000:]\n",
    "y_train, y_val = y_train[0:55000], y_train[55000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.2136 - accuracy: 0.6999 - val_loss: 0.5543 - val_accuracy: 0.8768\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.5125 - accuracy: 0.8655 - val_loss: 0.3549 - val_accuracy: 0.9134\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4015 - accuracy: 0.8895 - val_loss: 0.3001 - val_accuracy: 0.9232\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3575 - accuracy: 0.8992 - val_loss: 0.2705 - val_accuracy: 0.9256\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3328 - accuracy: 0.9053 - val_loss: 0.2518 - val_accuracy: 0.9306\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.3164 - accuracy: 0.9096 - val_loss: 0.2404 - val_accuracy: 0.9334\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3036 - accuracy: 0.9131 - val_loss: 0.2313 - val_accuracy: 0.9350\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2939 - accuracy: 0.9155 - val_loss: 0.2287 - val_accuracy: 0.9346\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2851 - accuracy: 0.9180 - val_loss: 0.2200 - val_accuracy: 0.9366\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2773 - accuracy: 0.9197 - val_loss: 0.2145 - val_accuracy: 0.9378\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater than 90%, pretty good! With a more complex mlp and more epochs you can do much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "\n",
    "Finiding a good rate is very import. Too large a learning rate and your training my never converge. Too low and training can take forever to converge. One has to find an optimal learning rate. Or even better, you don't need to have a constant learning rate. You can start with a large learning rate and then as soon as the training stops making fast progress you can reduce the learning rate. You can use what is called a learning rate scheduler to define the learning rate as a function of epoch. The learning rate scheduler can be passed to the fit function as a callback "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Scheduling\n",
    "\n",
    "lr = lr0 /(1 + (epoch / s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate lr0 drops by 1/2, 1/3, 1/4 .. after s steps\n",
    "def power_decay(lr0, s):\n",
    "    def power_decay_function(epoch):\n",
    "        return lr0/(1 + (epoch / s))\n",
    "    return power_decay_function\n",
    "\n",
    "power_decay_fn = power_decay(0.01,20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(power_decay_fn)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "             \n",
    "num_epochs = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Scheduling\n",
    "\n",
    "<mrow>lr = lr0 * 0.1**(epoch / s)</mrow>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "             \n",
    "num_epochs = 10\n",
    "hisotry = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Constant Sheduling\n",
    "\n",
    "With piecewise constant sheduling you can set a fixed learning rate for different epoch ranges.Most of the time I find piecewise sheduling simple and effective enough to acheive what I want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Two implementations for piecewise constant sheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "    \n",
    "    \n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "             \n",
    "num_epochs = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Overfiiting through dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_events):\n",
    "    noise, scale  = np.random.rand(2, num_events)\n",
    "    #Generate x values\n",
    "    x = scale * np.pi *2\n",
    "    #Generate y values\n",
    "    y = np.sin (1.2*x - 1.5) + (0.8* noise)\n",
    "    return x , y\n",
    "\n",
    "data_x, data_y  = generate_data(45)\n",
    "\n",
    "i1 = 15\n",
    "i2 = 30\n",
    "\n",
    "x_train , x_val, x_test = data_x[0:i1], data_x[i1:i2], data_x[i2:]\n",
    "y_train, y_val, y_test = data_y[0:i1], data_y[i1:i2], data_y[i2:]\n",
    "\n",
    "\n",
    "plt.plot(x_train, y_train, 'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple MLP to model the distribution. This is regression so no use of the sigmoid activation function on the output. Also we will use a different loss function, say mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\", input_dim=1))\n",
    "model.add(keras.layers.Dense(32, activation=\"relu\", input_dim=1))\n",
    "model.add(keras.layers.Dense(1, ))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "             \n",
    "    \n",
    "piecewise_constant_fn = piecewise_constant([100, 150, 300], [0.01, 0.0075, 0.005, 0.002])\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)  \n",
    "    \n",
    "num_epochs = 500\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', color='red', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prediction = model.predict(x_train)\n",
    "#Change the shape of data (make it 1D)\n",
    "y_train_prediction = np.squeeze(y_train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimation with truth\n",
    "\n",
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.plot(x_train, y_train_prediction, 'o', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = model.predict(x_test)\n",
    "#Change the shape of data (make it 1D)\n",
    "y_test_prediction = np.squeeze(y_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimation with truth\n",
    "\n",
    "plt.plot(x_test, y_test, 'o')\n",
    "plt.plot(x_test, y_test_prediction, 'o', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
