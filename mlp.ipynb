{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training a Multi Layered Perceptron (MLP) using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/olaiya/MLTutorialNotebooks/blob/master/mlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "- [1. Constructing a MLP with Tensorflow](#1.)\n",
    "    - [1.1 Import required libraries](#1.1)\n",
    "    - [1.2 Generate a dataset](#1.2)\n",
    "    - [1.3 Activation functions](#1.3)\n",
    "    - [1.4 Examples of activation functions in Tensorflow](#1.4)\n",
    "    - [1.5 Sequential API](#1.5)\n",
    "    - [1.6 Training on data](#1.6)\n",
    "    - [1.7 Functional API](#1.7)\n",
    "    - [1.8 Saving a model](#1.8)\n",
    "    - [1.9 Loading a saved model](#1.9)\n",
    "- [2. Image Classification with a MLP](#2.)\n",
    "- [3. Learning rate scheduling](#3.)\n",
    "    - [3.1 Power scheduling](#3.1)\n",
    "    - [3.2 Exponential scheduling](#3.2)\n",
    "    - [3.3 Piecewise constant scheduling](#3.3)\n",
    "- [4. MLPs for regression](#4.)\n",
    "- [5. Dropout](#5.)\n",
    "- [6. Exercise](#6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Contructing a MLP with Tensorflow <a name=\"1.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workbook we will use the python library Tensorflow to implement an MLP. We will implement MLPs for classification as a way of dipping into Tensorflow. We will also cover considerations for training such as batch sizes and learning rates as well as ways to avoid overfitting. We will also looking at the training loss output as well as saving and loading models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a code cell, click on the cell the press \"Shift + Enter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import required libraries <a name=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "#Want to use version of Tensorflow > 2.0\n",
    "print('Using Tensorflow version %s' % tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate a dataset <a name=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data\n",
    "\n",
    "Let's generate a dataset, consisting of two data types which we call signal and background. Each data type is normally generated around a point in the x-y plane. Distinguishing signal from backgorund in this case is a very simple problem. You could use PDFs and likelihoods to identify signal and backgound. However for the purpose of this tutorial we will build a very simply MLP classifier to identify events as signal or background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets\n",
    "num_events = 10000\n",
    "\n",
    "#Signal x and y mean values\n",
    "signal_mean = [1.0, 1.0]\n",
    "#Signal x and y values are uncorrelated\n",
    "signal_cov = [[1.0, 0.0],\n",
    "              [0.0, 1.0]]\n",
    "\n",
    "#Generate a training and validation sample\n",
    "signal_train = np.random.multivariate_normal(\n",
    "        signal_mean, signal_cov, num_events)\n",
    "signal_val = np.random.multivariate_normal(\n",
    "        signal_mean, signal_cov, num_events)\n",
    "\n",
    "#Background x and y mean values\n",
    "background_mean = [-1.0, -1.0]\n",
    "#Background x and y values are uncorrelated\n",
    "background_cov = [[1.0, 0.0],\n",
    "                  [0.0, 1.0]]\n",
    "\n",
    "#Generate a training and validation sample\n",
    "background_train = np.random.multivariate_normal(\n",
    "        background_mean, background_cov, num_events)\n",
    "background_val = np.random.multivariate_normal(\n",
    "        background_mean, background_cov, num_events)\n",
    "\n",
    "#Add the signal and background samples\n",
    "data_train = np.vstack([signal_train, background_train])\n",
    "labels_train = np.vstack([np.ones((num_events, 1)), np.zeros((num_events, 1))])\n",
    "\n",
    "#Add the signal and background samples\n",
    "data_val = np.vstack([signal_val, background_val])\n",
    "labels_val = np.vstack([np.ones((num_events, 1)), np.zeros((num_events, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the datasets generated\n",
    "range_ = ((-3, 3), (-3, 3))\n",
    "plt.figure(0, figsize=(8,4))\n",
    "plt.subplot(1,2,1); plt.title(\"Signal\")\n",
    "plt.xlabel(\"x\"), plt.ylabel(\"y\")\n",
    "plt.hist2d(signal_train[:,0], signal_train[:,1],\n",
    "        range=range_, bins=20, cmap=cm.coolwarm)\n",
    "plt.subplot(1,2,2); plt.title(\"Background\")\n",
    "plt.hist2d(background_train[:,0], background_train[:,1],\n",
    "        range=range_, bins=20, cmap=cm.coolwarm)\n",
    "plt.xlabel(\"x\"), plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a simple problem we can build a simple MLP to identify signal and background events \n",
    "\n",
    "An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that can use a nonlinear activation function\n",
    "\n",
    "<img src=\"images/mlp.png\" alt=\"mlp\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Activation Functions <a name=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Examples of activation functions in Tensorflow <a name=\"1.4\"></a>\n",
    "\n",
    "I use the tensorflow shorthand for activation functions such as activation=\"relu\". You can instead use activation=tf.nn.relu\n",
    "\n",
    "\n",
    "tf.nn.relu\n",
    "tf.nn.leaky_relu\n",
    "tf.nn.elu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Sequential API  <a name=\"1.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Tensorflows sequential API. We pass the layers to the API sequentially\n",
    "#Construct Neural Net\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\", input_dim=2))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "#Use model with leaky relu activation function\n",
    "#model = tf.keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Dense(100, activation=tf.nn.leaky_relu, input_dim=2))\n",
    "#model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to look at the architecture of your neural net to check it makes sense. Do you have the total number of parameters you would expect to have. \n",
    "\n",
    "Expected number of parameters: each input is connected to a node in the first hidden layer by a weight. So that is 2 inputs connected to 100 nodes = 200 weights. Each node has a bias, so 100 biases. Therefore that is 200 weights + 100 biases = 300 parameters at the first layer\n",
    "\n",
    "The 100 nodes are connected to 1 ouput node, so that is 100 weights. The output node has a bias to the number of parameters here is 100. Therefore the total number of parameters required to construct this neural net is 401\n",
    "\n",
    "Use the model's method .summary() to see a breakdown of your neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Training on data  <a name=\"1.6\"></a>\n",
    "\n",
    "Set loss function and optimiser. As this is a classification neural net we want to use binary cross entropy as the loss function. We will use Adam as the optimiser. The Adam optimiser is based on gradient decent but has a more sophisticated adaption of learning rates for parameters. Then you run the 'compile' method on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss function and optimiser\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on data. Define the dataset you want to train on and the validation set you want to validate the training against. Set the number of epochs (the number of iterations over the dataset). Set the batch size. This is a number/size of events smaller than your dataset that you run over and update the weights of your neural net. You run over multiples of batch sizes until you have run over all your events in your dataset. That is one epoch.  The batch size can actually be quite important. The main benefit of large batch sizes is that the training algorithm will see more instances per calculation. If you have the hardware large batch sizes are usually recommended. However large batch sizes can lead to instabilities. Particularly at the start of trainin.  A small batch size can be good if you are low on computer memory or want to avoid getting stuck in a local minima. Too small a batch size can make converging on optimal weighs slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "#Not worried about memory or local minima\n",
    "batchSize = len(data_train)\n",
    "\n",
    "#Train on data\n",
    "history = model.fit(data_train, labels_train,\n",
    "          validation_data=(data_val, labels_val),\n",
    "          batch_size=batchSize,\n",
    "          epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% accurracy. Not bad! Be careful with accuracy measurments. It is easy to measure high accuracy if your validation or test sample mainly has one category of data. Not the case here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "val_loss, val_acc = model.evaluate(data_val,  labels_val, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss of the neural net as a function of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', color='red', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Functional API  <a name=\"1.7\"></a>\n",
    "\n",
    "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "We can construct the above MLP model in Tensorlow using the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(100, activation=\"relu\")(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "model_fapi = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_fapi.summary()\n",
    "\n",
    "# Set loss function and optimiser\n",
    "model_fapi.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "num_epochs = 10\n",
    "#Only run with 10 epochs. Just a demonstration that functional API works\n",
    "#Not worried about memory or local minima\n",
    "batchSize = len(data_train)\n",
    "\n",
    "#Train on data\n",
    "history = model_fapi.fit(data_train, labels_train,\n",
    "          validation_data=(data_val, labels_val),\n",
    "          batch_size=batchSize,\n",
    "          epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Saving Models  <a name=\"1.8\"></a>\n",
    "\n",
    "What about saving the model. Also at what point do save the model. This is an ideal example where the loss is smooth and decreases monotonically. You'll find this is rarely the case. What I tend to do is save the model each time the validation loss reaches a minimum. You overtrain your neural net if you use the training loss as a metric for saving your model. Let's use a callback to save the model. A callback lets you specify a list of objects that will be called during training. We can use a checkpoint callback to save the model every time the validation loss reaches a new minimum. To start with we will save the weights of the model only, by specifying save_weights_only=True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Specify a directory to save model\n",
    "\n",
    "model_filename = 'simple_mlp_weights.h5'\n",
    "saved_model_directory = 'models'\n",
    "\n",
    "checkpoint_filename = os.path.join(saved_model_directory, model_filename)\n",
    "\n",
    "CHECK_FOLDER = os.path.isdir(saved_model_directory)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(saved_model_directory)\n",
    "    print(\"created folder : \", saved_model_directory)\n",
    "                \n",
    "#Create checkpoint\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filename,\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose = 1)\n",
    "\n",
    "        \n",
    "history = model.fit(data_train, labels_train,\n",
    "        validation_data=(data_val, labels_val),\n",
    "        batch_size=len(data_train),\n",
    "        epochs=num_epochs,\n",
    "        callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice your model continues learning from its previous parameters! Makes sense, you haven't reset the model parameters from the last training iteration\n",
    "\n",
    "So we have saved the weights of the model, so you will need the model before you can load the weights (next sub section). What if you want to save the model itself, so it can be recalled without any prior information. You can do this using the save method. Don't use thw .h5 file type extension when saving the full model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_filename = 'simple_mlp_fullModel'\n",
    "saved_model_directory = 'models'\n",
    "\n",
    "model_filename = os.path.join(saved_model_directory, full_model_filename)\n",
    "model.save(full_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Load a saved model <a name=\"1.9\"></a>\n",
    "\n",
    "Now we have saved the parameters we can load them at any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model weights\n",
    "\n",
    "model.load_weights(checkpoint_filename)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example model needed to be defined and the weights loaded. If you have the full model saved you can load it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load full model\n",
    "loaded_model = keras.models.load_model(full_model_filename)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image classification with a MLP  <a name=\"2.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the MNIST dataset, famous in machine learning examples. Specifically we can look at images of handwritten numbers 0-9 and use a MLP to identify them. This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the <a id='http://yann.lecun.com/exdb/mnist'>MNIST homepage</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "# Randomly look at one of the images\n",
    "random_ints =np.random.randint(len(x_test), size=1)\n",
    "image_index = random_ints[0]\n",
    "\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "\n",
    "print('Image label is: %i' % y_test[image_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an mlp to identify the images. The intenstiy of the pixels is represented as a value from 0 to 255. We should normalise the intensity! This is important! I your inputs have different scales, they will have disproportionate contributions to the loss. Scale your inputs to avoid this problem!\n",
    "\n",
    "Also, the image is a 2D array so we should flatten it to a 1D array so the values can be input into a MLP. Lets build a simple MLP with a hidden layer containing 128 nodes and an output layer of 10 nodes. Again we can work out how many parameters it would take to build this mlp. \n",
    "\n",
    "The total number of inputs required for the mlp is (an input for each pixel) 28 x 28  = 784\n",
    "\n",
    "Total number of weights to connect each input to each node in the first layer is 784 x 128 = 100352\n",
    "\n",
    "Total number of biases in the first layer is 128.\n",
    "\n",
    "Total number of parameters for the first layer is = 100352 + 128 = 100480\n",
    "\n",
    "Total number of parameters for the output layer is = number of weights + biases = (128 * 10) + 10 = 1290\n",
    "\n",
    "Total number of parameters =  101770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity of pixels ranges from 0-255. We need to normalise them\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "#Rather than instantiate the node and use the .add method to keep adding input,layers, outputs, etc we can just state\n",
    "#components separated by a comma during instantiation\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would expect some kind of sigmoid activation function to be applied to the output. However we let the loss function SparseCategoricalCrossentropy take the output and use it to calculate the loss to best attribute probabilities to each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity of pixels ranges from 0-255. We need to normalise them\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#Split into training and validation samples\n",
    "x_train, x_val = x_train[0:55000], x_train[55000:]\n",
    "y_train, y_val = y_train[0:55000], y_train[55000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater than 90%, pretty good! With a more complex mlp and more epochs you can do much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_val,  y_val, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Scheduling <a name=\"3.\"></a>\n",
    "\n",
    "Finiding a good rate is very import. Too large a learning rate and your training may never converge. Too low and training can take forever to converge. One has to find an optimal learning rate. Or even better, you don't need to have a constant learning rate. You can start with a large learning rate and then as soon as the training stops making fast progress you can reduce the learning rate. You can use what is called a learning rate scheduler to define the learning rate as a function of epoch. The learning rate scheduler can be passed to the fit function as a callback "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Power Scheduling <a name=\"3.1\"></a>\n",
    "\n",
    "lr = lr0 /(1 + (epoch / s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate lr0 drops by 1/2, 1/3, 1/4 .. after s steps\n",
    "def power_decay(lr0, s):\n",
    "    def power_decay_function(epoch):\n",
    "        return lr0/(1 + (epoch / s))\n",
    "    return power_decay_function\n",
    "\n",
    "power_decay_fn = power_decay(0.01,20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(power_decay_fn)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "             \n",
    "num_epochs = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exponential Scheduling <a name=\"3.2\"></a>\n",
    "\n",
    "<mrow>lr = lr0 * 0.1**(epoch / s)</mrow>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate drops exponentially\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "             \n",
    "num_epochs = 10\n",
    "hisotry = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Piecewise Constant Scheduling <a name=\"3.3\"></a>\n",
    "\n",
    "With piecewise constant sheduling you can set a fixed learning rate for different epoch ranges.Most of the time I find piecewise sheduling simple and effective enough to acheive what I want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Two implementations for piecewise constant sheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "    \n",
    "    \n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "             \n",
    "num_epochs = 10\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLPs for Regression <a name=\"4.\"></a>\n",
    "\n",
    "We can use MLPs to predict values. Simply remove the sigmoid activation function and change the loss function. You can pick Mean Square Error (MSE), Mean Average Error (MAE), whatever you like.  https://www.tensorflow.org/api_docs/python/tf/keras/losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_events):\n",
    "    \n",
    "    #Generate x values\n",
    "    x = np.linspace(-10, 30, num_events)\n",
    "    #Generate noise#\n",
    "    noise = np.random.rand(1, num_events)\n",
    "    #Generate y values (x squared distribution + noise)\n",
    "    y = (x  * x)\n",
    "    #Add noise\n",
    "    noise =  noise * y\n",
    "    y = y + noise\n",
    "    #Scale data\n",
    "    ymax = np.amax(y[0])\n",
    "    y = y / np.amax(y[0])\n",
    "    #Add noise\n",
    "    #noise = noise * amplitude\n",
    "    #y = y + noise\n",
    "    #Shuffle the data\n",
    "    shuffled_indices = np.arange(len(x))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    return x[shuffled_indices] , y[0][shuffled_indices]\n",
    "\n",
    "data_x, data_y  = generate_data(500)\n",
    "\n",
    "i1 = 400\n",
    "i2 = 450\n",
    "\n",
    "x_train , x_val, x_test = data_x[0:i1], data_x[i1:i2], data_x[i2:]\n",
    "y_train, y_val, y_test = data_y[0:i1], data_y[i1:i2], data_y[i2:]\n",
    "\n",
    "\n",
    "plt.plot(x_train, y_train, 'o', label='Data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple MLP to model the distribution. This is regression so no use of the sigmoid activation function on the output. Also we will use a different loss function, say mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, )\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "                 \n",
    "num_epochs = 200\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', color='red', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = model.predict(x_test)\n",
    "#Change the shape of data (make it 1D)\n",
    "y_test_prediction = np.squeeze(y_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimation with truth\n",
    "\n",
    "plt.plot(x_test, y_test, 'o', label='Test data')\n",
    "plt.plot(x_test, y_test_prediction, 'o', color='red', label='Model prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dropout <a name=\"5.\"></a>\n",
    "\n",
    "If your neural net tends to suffer from overtraining, using the dropout technique is a good way to combat overtraining. At every training step, every neuron in the layer has a probability p of being 'dropped out'. This means if the neuron is dropped out it will be entirely ignored during training. The hyperparameter p is called the dopout rate and is typically set between 10% and 50%. You can also apply dropout to the input layer.\n",
    "\n",
    "We can construct a more complex neural net for our dataset and add dropout. The purpose is only for demonstrating the implentation of dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(20, activation='relu'),\n",
    "  tf.keras.layers.Dropout(rate=0.3),\n",
    "  tf.keras.layers.Dense(20, activation='relu'),\n",
    "  tf.keras.layers.Dropout(rate=0.3),\n",
    "  tf.keras.layers.Dense(1, )\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "                 \n",
    "num_epochs = 200\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = model.predict(x_test)\n",
    "#Change the shape of data (make it 1D)\n",
    "y_test_prediction = np.squeeze(y_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimation with truth\n",
    "\n",
    "plt.plot(x_test, y_test, 'o', label='Test data')\n",
    "plt.plot(x_test, y_test_prediction, 'o', color='red', label='Model prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise <a name=\"6.\"></a>\n",
    "\n",
    "Rerun the image classification MLP with some modifications to see if you can improve the accuracy. Play with the number of nodes, layers, change the activation functions, the loss function, add learning rate scheduling, maybe dropout.  Don't make your neural net too large. We do have limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
